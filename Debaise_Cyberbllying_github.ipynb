{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LgSvtSM55UB"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import nltk\n",
        "import json\n",
        "import torch.nn.functional as F\n",
        "import datetime\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report,f1_score\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW,AutoModel, BertTokenizerFast,BertModel,RobertaTokenizer, RobertaModel,RobertaTokenizerFast\n",
        "from nltk import word_tokenize\n",
        "from dataset import InstagramDataset,VineDataset,Swearwords\n",
        "from psutil import virtual_memory\n",
        "from nltk import word_tokenize\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.Check the device"
      ],
      "metadata": {
        "id": "2QQr6kNDRt__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available and torch.cuda.device_count()>0:\n",
        "  device=torch.device(\"cuda\")\n",
        "  print(\"there are %d GPU(S) avaliable\" % torch.cuda.device_count())\n",
        "else:\n",
        "  print(\"NO GPU avaliable\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnW7UveTR9pu",
        "outputId": "e8b3c7bd-15c4-4824-9fae-eadd124052ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NO GPU avaliable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n"
      ],
      "metadata": {
        "id": "zkqgMRv_SFm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Configuration"
      ],
      "metadata": {
        "id": "2iCIuRzw1GAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 16\n",
        "        self.num_epochs = 4\n",
        "        self.lr = 2e-5\n",
        "        self.t = 15.0\n",
        "\n",
        "        self.max_sents=140\n",
        "        self.max_document_length=512\n",
        "        self.modelname=\"Roberta\"\n",
        "\t      #self.modelname=\"Bert\"\n",
        "\n",
        "        self.debias = True\n",
        "        self.hidden_layer = False\n",
        "        self.constraint=0.2\n",
        "        self.test_size=0.2\n",
        "        self.random_state=121\n",
        "\n",
        "        self.dropout = 0.3\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "iHvjLXtjqZri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQJPH-OZ7Z6u"
      },
      "source": [
        "# 2.Load Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTJtA451yFui"
      },
      "source": [
        "##2.1 Load Vine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h8OiIo09CRD"
      },
      "outputs": [],
      "source": [
        "dataDict_vine=[]\n",
        "userinfo_vine = {}\n",
        "vine_data=VineDataset('./data/vine/vine_labeled_cyberbullying_data.csv')\n",
        "vine_profile_data=VineDataset('./data/vine/vine_users_data.json')\n",
        "\n",
        "with open(vine_data, 'r',encoding='unicode_escape') as f:\n",
        "    reader = csv.DictReader((line.replace('\\0','') for line in f))\n",
        "    for row in reader:\n",
        "        dataDict_vine.append(row)\n",
        "\n",
        "for line in vine_profile_data.readlines():\n",
        "    rr = json.loads(line)\n",
        "    userinfo_vine[rr['username'].lower()]=rr['description'].lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SgRhXVH-JTF"
      },
      "outputs": [],
      "source": [
        "# Convert time to hours\n",
        "def timeconvert(timestr,start_time):\n",
        "    ifpm=False\n",
        "    ntp=datetime.datetime.strptime(timestr, \"%Y-%m-%d %H:%M:%S\")\n",
        "    try:\n",
        "        otp=datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
        "    except:\n",
        "        start_time='2'+start_time\n",
        "        otp=datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
        "    delta=ntp-otp\n",
        "    hours=delta.days*24+delta.seconds/3600\n",
        "    return hours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdJiVtb6-fph",
        "outputId": "8bfe733a-e4c2-4b14-a56f-225ae5dc715b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Remove redundant session info\n",
        "removeList=['_golden','_unit_state','_unit_id','_trusted_judgments','_last_judgment_at','img_url']\n",
        "for row in dataDict_vine:\n",
        "    row['creationtime']=row['creationtime'].split('posted at:')[-1].strip()\n",
        "    row['creationtime']=row['creationtime'].replace('.000000','')\n",
        "    row['creationtime']=row['creationtime'].replace('T',' ')\n",
        "    for keys in list(row):\n",
        "        if (keys in removeList) or (keys[:4]=='colu' and row[keys]=='empty'):\n",
        "            del row[keys]\n",
        "        elif keys[:4]=='colu':\n",
        "            row[keys]=row[keys].replace('<font color=\"#0066CC\">',\"\")\n",
        "            row[keys]=row[keys].replace('</font>::',\"&&&&&\")\n",
        "            row[keys]=row[keys].replace('(created at:','(created_at:')\n",
        "            row[keys]=row[keys].split('(created_at:')\n",
        "            if len(row[keys])>1:\n",
        "                row[keys]=[row[keys][0].strip(),row[keys][1].strip(')')]\n",
        "                row[keys][1]=row[keys][1].replace('.000000','')\n",
        "                row[keys]=[row[keys][0].split(\"&&&&&\")[0],row[keys][0].split(\"&&&&&\")[1],row[keys][1].replace('T',' ')]\n",
        "                row[keys][0]=row[keys][0].lower()\n",
        "                row[keys][1]=row[keys][1].lower()\n",
        "                new_str=re.sub(r'[\\x80-\\xFF]+','',row[keys][1])\n",
        "                if new_str!=row[keys][1]:\n",
        "                    row[keys][1]=re.sub('\\_*','',new_str)\n",
        "                row[keys][1]=word_tokenize(row[keys][1])\n",
        "            else:\n",
        "                del row[keys]\n",
        "    try:\n",
        "        datetime.datetime.strptime(row['creationtime'], \"%Y-%m-%d %H:%M:%S\")\n",
        "    except:\n",
        "        for i in range(10):\n",
        "            try:\n",
        "                row['creationtime']=row['column'+str(i)][2]\n",
        "                break\n",
        "            except:\n",
        "                pass\n",
        "    for keys in list(row):\n",
        "        if keys[:4]=='colu':\n",
        "            row[keys][2]=timeconvert(row[keys][2],row['creationtime'])\n",
        "    row['likecount']=row['likecount'].split('\\n\\n ')[0]\n",
        "    row['username']=row['username'].replace('<font color=\"#0066CC\">',\"\")\n",
        "    row['username']=row['username'].replace('</font>',\"\")\n",
        "    new_cptn=re.sub(r'[\\x80-\\xFF]+','',row['mediacaption'])\n",
        "    if new_cptn!=row['mediacaption']:\n",
        "        row['mediacaption']=re.sub('\\_*','',new_cptn)\n",
        "    row['mediacaption']=word_tokenize(row['mediacaption'].lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHy-f2bO-qZ6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Vine data ready\n",
        "session_labels_vine=[]\n",
        "session_tokens_vine=[]\n",
        "session_histories=[]\n",
        "session_times=[]\n",
        "for row in dataDict_vine:\n",
        "    if row['question1']=='noneAgg' and row['question2']=='noneBll':\n",
        "        session_labels_vine.append(0)\n",
        "    elif row['question1']=='aggression' and row['question2']=='noneBll':\n",
        "        session_labels_vine.append(0)\n",
        "    else:\n",
        "        session_labels_vine.append(1)\n",
        "\n",
        "    row_tokens=[]\n",
        "    row_times=[]\n",
        "    row_history=[]\n",
        "    owner_ut=[row['username']]+row['mediacaption']\n",
        "    row_tokens.append(owner_ut)\n",
        "    if row['username'] in userinfo_vine.keys():\n",
        "        row_history.append(userinfo_vine[row['username']])\n",
        "    else:\n",
        "        row_history.append([])\n",
        "    row_times.append(0)\n",
        "    for keys in list(row):\n",
        "        if keys[:4]=='colu':\n",
        "            row_tokens.append([row[keys][0]]+row[keys][1])\n",
        "            row_times.append(row[keys][2])\n",
        "    sorted_row_times=[]\n",
        "    sorted_row_tokens=[]\n",
        "\n",
        "    mintime=row_times[np.argsort(row_times)[0]]\n",
        "    for i in np.argsort(row_times):\n",
        "        if mintime<0:\n",
        "            sorted_row_times.append(row_times[i]-mintime)\n",
        "        else:\n",
        "            sorted_row_times.append(row_times[i])\n",
        "        sorted_row_tokens.append(row_tokens[i])\n",
        "\n",
        "    sorted_row_times=sorted_row_times[:config.max_sents]+(config.max_sents-len(sorted_row_times))*[0]\n",
        "    session_times.append(sorted_row_times)\n",
        "    session_tokens_vine.append(sorted_row_tokens)\n",
        "    session_histories.append(row_history)\n",
        "\n",
        "comments_list_vine=[[\" \".join(x)  for x in y] for y in session_tokens_vine]\n",
        "data=[\" \".join(x) for x in comments_list_vine]\n",
        "labels=session_labels_vine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2KAG3p76nPp"
      },
      "source": [
        "##2.2 Load Instagram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49uXRQX-pv-1"
      },
      "outputs": [],
      "source": [
        "In_data_1=InstagramDataset('./data/instagram/sessions_0plus_to_10_metadata.csv')\n",
        "In_data_2=InstagramDataset('./data/instagram/sessions_10plus_to_40_metadata.csv')\n",
        "In_data_3=InstagramDataset('./data/instagram/sessions_40plus_metadata.csv')\n",
        "dataDict=[]\n",
        "with open(In_data_1, 'r',encoding='unicode_escape') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        dataDict.append(row)\n",
        "with open(In_data_2, 'r',encoding='unicode_escape') as f:\n",
        "    reader = csv.DictReader((line.replace('\\0','') for line in f))\n",
        "    for row in reader:\n",
        "        dataDict.append(row)\n",
        "with open(In_data_3, 'r',encoding='unicode_escape') as f:\n",
        "    reader = csv.DictReader((line.replace('\\0','') for line in f))\n",
        "    for row in reader:\n",
        "        dataDict.append(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHysv1AOySi7"
      },
      "outputs": [],
      "source": [
        "# Convert time to hours\n",
        "def timeconvert(timestr,start_time):\n",
        "    ifpm=False\n",
        "    ntp=datetime.datetime.strptime(timestr, \"%Y-%m-%d %H:%M:%S\")\n",
        "    try:\n",
        "        otp=datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
        "    except:\n",
        "        start_time='2'+start_time\n",
        "        otp=datetime.datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
        "    delta=ntp-otp\n",
        "    hours=delta.days*24+delta.seconds/3600\n",
        "    return hours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaXjq5TARgYP"
      },
      "outputs": [],
      "source": [
        "# Remove redundant session info\n",
        "removeList=['_golden','_unit_state','_unit_id','_trusted_judgments','_last_judgment_at','img_url']\n",
        "for row in dataDict:\n",
        "    row['cptn_time']=row['cptn_time'].split('Media posted at ')[-1].strip()\n",
        "    for keys in list(row):\n",
        "        if (keys in removeList) or (keys[:4]=='clmn' and row[keys]=='empety'):\n",
        "            del row[keys]\n",
        "        elif keys[:4]=='clmn':\n",
        "            row[keys]=row[keys].replace('<font color=\"#0066CC\">',\"\")\n",
        "            row[keys]=row[keys].replace('</font>',\"\")\n",
        "            row[keys]=row[keys].replace('(created at:','(created_at:')\n",
        "            row[keys]=row[keys].split('(created_at:')\n",
        "            row[keys]=[row[keys][0].strip(),row[keys][1].strip(')')]\n",
        "            row[keys]=[row[keys][0].split('   ')[0],row[keys][0].split('   ')[1],row[keys][1]]\n",
        "            row[keys][0]=row[keys][0].lower()\n",
        "            row[keys][1]=row[keys][1].lower()\n",
        "            new_str=re.sub(r'[\\x80-\\xFF]+','',row[keys][1])\n",
        "            if new_str!=row[keys][1]:\n",
        "                row[keys][1]=re.sub('\\_*','',new_str)\n",
        "            row[keys][1]=word_tokenize(row[keys][1])\n",
        "            row[keys][2]=timeconvert(row[keys][2],row['cptn_time'])\n",
        "    row['likes']=row['likes'].split('\\n\\n ')[0]\n",
        "    row['owner_id']=row['owner_id'].replace('<font color=\"#0066CC\">',\"\")\n",
        "    row['owner_id']=row['owner_id'].replace('</font>  ',\"\")\n",
        "    new_cptn=re.sub(r'[\\x80-\\xFF]+','',row['owner_cmnt'])\n",
        "    if new_cptn!=row['owner_cmnt']:\n",
        "        row['owner_cmnt']=re.sub('\\_*','',new_cptn)\n",
        "    row['owner_cmnt']=word_tokenize(row['owner_cmnt'].lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAy7DznqR0Ue"
      },
      "outputs": [],
      "source": [
        "# Construct input session data according to time\n",
        "session_labels_in=[]\n",
        "session_tokens_in=[]\n",
        "session_histories=[]\n",
        "session_times=[]\n",
        "for row in dataDict:\n",
        "    if row['question1']=='noneAgg' and row['question2']=='noneBll':\n",
        "        session_labels_in.append(0)\n",
        "    elif row['question1']=='aggression' and row['question2']=='noneBll':\n",
        "        session_labels_in.append(0)\n",
        "    else:\n",
        "        session_labels_in.append(1)\n",
        "\n",
        "    row_tokens=[]\n",
        "    row_times=[]\n",
        "    row_history=[]\n",
        "\n",
        "    for keys in list(row):\n",
        "        if keys[:4]=='clmn':\n",
        "            row_tokens.append([row[keys][0]]+row[keys][1])\n",
        "            row_times.append(row[keys][2])\n",
        "\n",
        "    sorted_row_times=[]\n",
        "    sorted_row_tokens=[]\n",
        "    sorted_row_history=[]\n",
        "\n",
        "    # It returns an array of indices of the same shape as arr that  would sort the array.\n",
        "    mintime=row_times[np.argsort(row_times)[0]]\n",
        "\n",
        "    for i in np.argsort(row_times):\n",
        "        if mintime<0:\n",
        "            sorted_row_times.append(row_times[i]-mintime)\n",
        "        else:\n",
        "            sorted_row_times.append(row_times[i])\n",
        "\n",
        "        sorted_row_tokens.append(row_tokens[i])\n",
        "    session_tokens_in.append(sorted_row_tokens)\n",
        "comments_list_in=[[\" \".join(x)  for x in y] for y in session_tokens_in]\n",
        "#Data ready!\n",
        "data=[\" \".join(x) for x in comments_list_in]\n",
        "labels=session_labels_in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwcR2B2YyLgB"
      },
      "source": [
        "##3.3 Swear words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezn46WWn7aJG"
      },
      "outputs": [],
      "source": [
        "Swearwords_van=Swearwords('./data/swearwords/swear_van.txt')\n",
        "Swearwords_google=Swearwords('./data/swearwords/swear_google.txt')\n",
        "swearwords=[]\n",
        "with open(Swearwords_van) as f:\n",
        "    swearwords = f.read().splitlines()\n",
        "\n",
        "with open(Swearwords_google) as f:\n",
        "    swearwords=swearwords+ f.read().splitlines()\n",
        "\n",
        "#remove redundant and space\n",
        "swearwords=list(set(swearwords))\n",
        "swearwords.remove('')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Define Base models"
      ],
      "metadata": {
        "id": "JlCUA8xeTqPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Loaddata(train_text, train_labels,test_text,test_labels):\n",
        "  X_test=test_text\n",
        "  Y_test=test_labels\n",
        "  X_train, X_val, Y_train, Y_val = train_test_split( train_text, train_labels, random_state=121, test_size=0.10)\n",
        "\n",
        "  if config.modelname==\"Bert\":\n",
        "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "  if config.modelname==\"Roberta\":\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "\n",
        "  # tokenize and encode sequences in the training set\n",
        "  tokens_train = tokenizer.batch_encode_plus(\n",
        "    X_train,\n",
        "    max_length = config.max_document_length,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        "  )\n",
        "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "  train_y = torch.tensor(Y_train)\n",
        "\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "  tokens_val = tokenizer.batch_encode_plus(\n",
        "    X_val,\n",
        "    max_length = config.max_document_length,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        "  )\n",
        "  val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "  val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "  val_y = torch.tensor(Y_val)\n",
        "\n",
        "  tokens_test = tokenizer.batch_encode_plus(\n",
        "    X_test,\n",
        "    max_length = config.max_document_length,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        "  )\n",
        "\n",
        "  test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "  test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "  test_y = torch.tensor(Y_test)\n",
        "  test_index = torch.tensor(Y_test)\n",
        "\n",
        "  train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=config.batch_size)\n",
        "\n",
        "  val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=config.batch_size)\n",
        "\n",
        "  test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "  test_sampler = SequentialSampler(test_seq)\n",
        "  test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=config.batch_size)\n",
        "\n",
        "  return train_dataloader,val_dataloader,test_dataloader,X_train,Y_train"
      ],
      "metadata": {
        "id": "hXgC4e-TURRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Bert"
      ],
      "metadata": {
        "id": "KDW--P94angw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihcdQJ5kdOh4"
      },
      "outputs": [],
      "source": [
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertEncoder, self).__init__()\n",
        "        self.encoder = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        outputs = self.encoder(x, attention_mask=mask)\n",
        "        feat = outputs[1]\n",
        "        feat_hidden = outputs[2]\n",
        "\n",
        "        return feat,feat_hidden\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, dropout=config.dropout):\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "\n",
        "       # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "        #softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "       # apply softmax activation\n",
        "        out = self.softmax(x)\n",
        "        return out\n",
        "\n",
        "    def init_bert_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 Roberta"
      ],
      "metadata": {
        "id": "4qJpNhfxcZ4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RobertaEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaEncoder, self).__init__()\n",
        "        self.encoder = RobertaModel.from_pretrained('roberta-base',output_hidden_states = True)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        outputs = self.encoder(x, attention_mask=mask)\n",
        "        feat = outputs[1]\n",
        "        feat_hidden = outputs[2]\n",
        "\n",
        "        return feat,feat_hidden\n",
        "\n",
        "\n",
        "class RobertaClassifier(nn.Module): #768->512->2+BN\n",
        "    def __init__(self, dropout=config.dropout):\n",
        "        super(RobertaClassifier, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "\n",
        "       # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        # dense layer 2 (Output layer)\n",
        "        self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "        #softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pass the inputs to the model\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "         # output layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "       # apply softmax activation\n",
        "        out = self.softmax(x)\n",
        "        return out\n",
        "\n",
        "    def init_bert_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()"
      ],
      "metadata": {
        "id": "-BBZijm1b_5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Define Training"
      ],
      "metadata": {
        "id": "h8u0fpi7cWWY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yfWmq2MG7gd"
      },
      "outputs": [],
      "source": [
        "##Get the hiddent states\n",
        "def get_newfeat(feat_hiddent,layerindx,bachsize):\n",
        "  sec_vec=torch.mean(feat_hiddent[layerindx][0], dim=0).reshape(1,768)\n",
        "  if bachsize==1:\n",
        "    return sec_vec\n",
        "  for i in range(1,bachsize):\n",
        "    sec_vec=torch.cat((sec_vec,torch.mean(feat_hiddent[layerindx][i], dim=0).reshape(1,768)),dim=0)\n",
        "  return sec_vec\n",
        "\n",
        "#Use CUDA if it's available.\"\"\"\n",
        "def make_cuda(tensor):\n",
        "    if torch.cuda.is_available():\n",
        "        tensor = tensor.cuda()\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def countbias_train(w,fp,fn,tp,tn,data):\n",
        "\n",
        "  FPW=sum([1 for i in fp if w in data[i]])\n",
        "\n",
        "  TNW=sum([1 for i in fn if w in data[i]])\n",
        "\n",
        "  FNW=sum([1 for i in tn if w in data[i]])\n",
        "\n",
        "  TPW=sum([1 for i in tp if w in data[i]])\n",
        "\n",
        "  if FPW+TNW==0 or FNW+TPW==0:\n",
        "    return 0,0\n",
        "  else:\n",
        "    #print(FPW,TNW,FNW,TPW)\n",
        "    return(abs(((len(fp)/(len(fp)+len(tn)))-(FPW/(FPW+TNW)))),abs((len(fn)/(len(fn)+len(tp)))-(FNW/(FNW+TPW)))) #FPD, FND"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Adversarialdata(data):\n",
        "  train_x_text_clear=[]\n",
        "  for x in data:\n",
        "    for swear in swearwords:\n",
        "      if swear in x:\n",
        "        x=x.replace(swear,\"\")\n",
        "    train_x_text_clear.append(x)\n",
        "\n",
        "  if config.model_name==\"Bert\":\n",
        "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "  if config.model_name==\"Roberta\":\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "\n",
        "  tokens_train_clear = tokenizer.batch_encode_plus(\n",
        "    train_x_text_clear,\n",
        "    max_length = config.max_document_length,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        "  )\n",
        "\n",
        "  train_seq_clear = torch.tensor(tokens_train_clear['input_ids'])\n",
        "  train_mask_clear = torch.tensor(tokens_train_clear['attention_mask'])\n",
        "  train_y = torch.tensor(y_train)\n",
        "  cosin_y=torch.tensor([1]*len(train_y))\n",
        "  train_data_clear = TensorDataset(train_seq_clear, train_mask_clear, cosin_y)\n",
        "  train_sampler_clear = SequentialSampler(train_data_clear)\n",
        "  train_dataloader_clear = DataLoader(train_data_clear,shuffle=False,sampler=train_sampler_clear, batch_size=config.batch_size)\n",
        "  return train_dataloader_clear"
      ],
      "metadata": {
        "id": "2PijhxUmkwU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQLZxpsv8D4h"
      },
      "source": [
        "## 4.1 Train_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z3tn5gFok4r"
      },
      "outputs": [],
      "source": [
        "def train_base(encoder, classifier, train_dataloader,val_dataloader,epoch_num):\n",
        "    best_valid_loss = float('inf')\n",
        "    optimizer = AdamW(list(encoder.parameters()) + list(classifier.parameters()),\n",
        "                       lr=2e-5)\n",
        "    CELoss = nn.CrossEntropyLoss()\n",
        "    encoder.train()\n",
        "    classifier.train()\n",
        "    for epoch in range(epoch_num):\n",
        "        loss=0\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            sent_id, mask, labels = batch\n",
        "            # zero gradients for optimizer\n",
        "            optimizer.zero_grad()\n",
        "            feat,feat_hidden = encoder(sent_id, mask)\n",
        "\n",
        "            preds = classifier(feat)\n",
        "            cls_loss = CELoss(preds, labels)\n",
        "            cls_loss.backward()\n",
        "            loss+=cls_loss\n",
        "\n",
        "            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            # print step info\n",
        "            if (step + 1) % 50 == 0:\n",
        "                print(\"Epoch [%.2d/%.2d] Step [%.3d/%.3d]: cls_loss=%.4f\"\n",
        "                      % (epoch + 1,\n",
        "                        2,\n",
        "                         step + 1,\n",
        "                         len(train_dataloader),\n",
        "                         cls_loss.item()))\n",
        "        print(\"Average Loss from training: \"+str(loss/len(train_dataloader)))\n",
        "        acc, preds,valid_loss,real=evaluate(encoder, classifier, val_dataloader)\n",
        "        if valid_loss < best_valid_loss:\n",
        "          best_valid_loss = valid_loss\n",
        "          # save final model\n",
        "          torch.save(encoder.state_dict(), 'source-encoder.pt')\n",
        "          torch.save(classifier.state_dict(), 'source-classifier.pt')\n",
        "\n",
        "    encoder.load_state_dict(torch.load('source-encoder.pt'))\n",
        "    classifier.load_state_dict(torch.load('source-classifier.pt'))\n",
        "    return encoder, classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJs4yagDvfjF"
      },
      "source": [
        "##4.2 Train_debias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALdksd2muAUq"
      },
      "outputs": [],
      "source": [
        "def train_debias(encoder, classifier, train_dataloader, train_dataloader_clear,val_dataloader,layer):\n",
        "    best_valid_loss = float('inf')\n",
        "    max_bias=float('inf')\n",
        "    best_f1=0\n",
        "    fpd=0\n",
        "    fnd=0\n",
        "    Tar=torch.tensor([1,-1])\n",
        "    T = 20\n",
        "\n",
        "    optimizer = AdamW(list(encoder.parameters()) + list(classifier.parameters()),\n",
        "                       lr=2e-5)\n",
        "\n",
        "    CELoss = nn.CrossEntropyLoss()\n",
        "    KLDivLoss = nn.KLDivLoss(reduction='batchmean')\n",
        "    cosloss = nn.CosineEmbeddingLoss(reduction='none')\n",
        "    encoder.train()\n",
        "    classifier.train()\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        loss=0\n",
        "\n",
        "        data_zip = enumerate(zip(train_dataloader, train_dataloader_clear))\n",
        "        for step, ((reviews_src, src_mask, labels), (reviews_tgt, tgt_mask, cosin_y)) in data_zip:\n",
        "\n",
        "            reviews_src = make_cuda(reviews_src)\n",
        "            src_mask = make_cuda(src_mask)\n",
        "\n",
        "            reviews_tgt = make_cuda(reviews_tgt)\n",
        "            tgt_mask = make_cuda(tgt_mask)\n",
        "\n",
        "            labels = make_cuda(labels)\n",
        "            cosin_y=make_cuda(cosin_y)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            feat_src_tgt, feat_src_tgt_hidden= encoder(reviews_src, src_mask)\n",
        "            feat_tgt,feat_tgt_hidden= encoder(reviews_tgt, tgt_mask)\n",
        "            if config.hidden_layer :\n",
        "              feat_src_tgt=get_newfeat(feat_src_tgt_hidden,layer,len(feat_src_tgt))\n",
        "              feat_tgt=get_newfeat(feat_tgt_hidden,layer,len(feat_tgt))\n",
        "\n",
        "            preds = classifier(feat_src_tgt)\n",
        "            cls_loss = CELoss(preds, labels)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                src_prob = F.softmax(classifier(feat_src_tgt) / T, dim=-1)\n",
        "            tgt_prob = F.log_softmax(classifier(feat_tgt) / T, dim=-1)\n",
        "\n",
        "\n",
        "            cos_loss = cosloss(feat_tgt, feat_src_tgt,cosin_y)\n",
        "\n",
        "\n",
        "            cls_loss=cls_loss+cos_loss+config.constraint*(fpd+fnd)\n",
        "            cls_loss.backward()\n",
        "            loss+=cls_loss\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # print step info\n",
        "            if (step + 1) % 50 == 0:\n",
        "                print(\"Epoch [%.2d/%.2d] Step [%.3d/%.3d]: cls_loss=%.4f\"\n",
        "                      % (epoch + 1,\n",
        "                        2,\n",
        "                         step + 1,\n",
        "                         len(train_dataloader),\n",
        "                         cls_loss.item()))\n",
        "\n",
        "        print(\"Average Loss from training: \"+str(loss/len(val_dataloader)))\n",
        "        acc, preds,valid_loss,real=evaluate(encoder, classifier, val_dataloader)\n",
        "        #acc, preds,valid_loss,real=evaluate(encoder, classifier, val_dataloader)\n",
        "        preds=[int(x.cpu().numpy()) for x in preds]\n",
        "        real=[int(x.cpu().numpy()) for x in real]\n",
        "        fpd=0\n",
        "        fnd=0\n",
        "        fp=[i for i in range(len(preds)) if preds[i]==1 and real[i]==0]\n",
        "        fn=[i for i in range(len(preds)) if preds[i]==0 and real[i]==1]\n",
        "        tp=[i for i in range(len(preds)) if preds[i]==1 and real[i]==1]\n",
        "        tn=[i for i in range(len(preds)) if preds[i]==0 and real[i]==0]\n",
        "        for w in swearwords:\n",
        "          x,y= countbias_train(w,fp,fn,tp,tn,val_dataloader)\n",
        "          fpd+=x\n",
        "          fnd+=y\n",
        "\n",
        "        if valid_loss < best_valid_loss :\n",
        "          best_valid_loss = valid_loss\n",
        "          torch.save(encoder.state_dict(), 'source-encoder.pt')\n",
        "          torch.save(classifier.state_dict(), 'source-classifier.pt')\n",
        "        #Jump when the loss threshold is reached\n",
        "        if valid_loss<config.t and epoch>0: break\n",
        "\n",
        "    encoder.load_state_dict(torch.load('source-encoder.pt'))\n",
        "    classifier.load_state_dict(torch.load('source-classifier.pt'))\n",
        "    return encoder, classifier,valid_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebMZD84VNFN-"
      },
      "source": [
        "##4.3 Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvyfiqRnysHi"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, classifier, data_loader,layer):\n",
        "    encoder.eval()\n",
        "    classifier.eval()\n",
        "\n",
        "    # init loss and accuracy\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    pred=[]\n",
        "    real=[]\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for batch in data_loader:\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        real+=labels\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feat,feat_hidden = encoder(sent_id, mask)\n",
        "\n",
        "            if config.hidden_layer:\n",
        "              newfeat=get_newfeat(feat_hidden,layer,len(feat))\n",
        "            else:\n",
        "              newfeat=feat\n",
        "\n",
        "            preds = classifier(newfeat)\n",
        "        loss += criterion(preds, labels).item()\n",
        "        pred_cls = preds.data.max(1)[1]\n",
        "        acc += pred_cls.eq(labels.data).cpu().sum().item()\n",
        "        pred+=pred_cls\n",
        "\n",
        "\n",
        "    loss /= len(data_loader)\n",
        "    acc /= len(data_loader.dataset)\n",
        "\n",
        "    print(\"Avg Loss = %.4f, Avg Accuracy = %.4f\" % (loss, acc))\n",
        "\n",
        "    return acc,pred,loss,real"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KC30KpW2JPe"
      },
      "source": [
        "#5.Count bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUU6xgpykwJm"
      },
      "outputs": [],
      "source": [
        "def count_fp_fn_tp_tn(predictions_list,target_list):\n",
        "  FP=[i for i in range(len(predictions_list)) if predictions_list[i]==1 and target_list[i]==0]\n",
        "  FN=[i for i in range(len(predictions_list)) if predictions_list[i]==0 and target_list[i]==1]\n",
        "  TP=[i for i in range(len(predictions_list)) if predictions_list[i]==1 and target_list[i]==1]\n",
        "  TN=[i for i in range(len(predictions_list)) if predictions_list[i]==0 and target_list[i]==0]\n",
        "  return FP,TN,FN,TP\n",
        "\n",
        "def countbias(word,FP,TN,FN,TP,data):\n",
        "  FPW=sum([1 for i in FP if word in data[i]])\n",
        "  TNW=sum([1 for i in TN if word in data[i]])\n",
        "  FNW=sum([1 for i in FN if word in data[i]])\n",
        "  TPW=sum([1 for i in TP if word in data[i]])\n",
        "  if FPW+TNW==0 or FNW+TPW==0:\n",
        "    return 0,0\n",
        "  else:\n",
        "    return(abs(((len(FP)/(len(FP)+len(TN)))-(FPW/(FPW+TNW)))),abs((len(FN)/(len(FN)+len(TP)))-(FNW/(FNW+TPW)))) #FPD, FND\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.Running"
      ],
      "metadata": {
        "id": "Hein8RLQ0jpl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_iZRpXMgnGS"
      },
      "outputs": [],
      "source": [
        "train_x_text,test_x_text,train_labels,test_labels= train_test_split(data,labels,random_state=config.random_state, test_size=config.test_size)\n",
        "train_dataloader,val_dataloader,test_dataloader,x_train,y_train=Loaddata(train_x_text,train_labels,test_x_text,test_labels)\n",
        "if config.modelname==\"Bert\":\n",
        "   train_encoder = BertEncoder()\n",
        "   train_classifier= BertClassifier()\n",
        "\n",
        "if config.modelname==\"Robert\":\n",
        "   train_encoder = RobertaEncoder()\n",
        "   train_classifier= RobertaClassifier()\n",
        "\n",
        "train_encoder = train_encoder.to(device)\n",
        "train_classifier = train_classifier.to(device)\n",
        "\n",
        "if config.debias:\n",
        "  train_dataloader_clear=get_Adversarialdata(x_train)\n",
        "  if config.hidden_layer:\n",
        "    best_loss = float('inf')\n",
        "    bestlayer=-1\n",
        "    for i in range(1,13):\n",
        "      train_encoder, train_classifier,loss = train_debias(train_encoder, train_classifier, train_dataloader,train_dataloader_clear,val_dataloader,i)\n",
        "      if best_loss>loss:\n",
        "        best_loss=loss\n",
        "        torch.save(train_encoder.state_dict(), 'encoder.pt')\n",
        "        torch.save(train_classifier.state_dict(), 'classifier.pt')\n",
        "        bestlayer=i\n",
        "    train_encoder.load_state_dict(torch.load('encoder.pt'))\n",
        "    train_classifier.load_state_dict(torch.load('classifier.pt'))\n",
        "  else:\n",
        "       train_encoder, train_classifier,_ = train_debias(train_encoder, train_classifier, train_dataloader,train_dataloader_clear,val_dataloader,i)\n",
        "else:\n",
        "    train_encoder, train_classifier =train_base(train_encoder, train_classifier, train_dataloader,val_dataloader,config.num_epochs)\n",
        "\n",
        "acc,preds,loss,real=evaluate(train_encoder, train_classifier, test_dataloader,bestlayer)\n",
        "preds=[int(x.cpu().numpy()) for x in preds]\n",
        "real=[int(x.cpu().numpy()) for x in real]\n",
        "print(classification_report(real, preds))\n",
        "\n",
        "#count bias\n",
        "FPD=[]\n",
        "FND=[]\n",
        "FP,TN,FN,TP=count_fp_fn_tp_tn(preds,real)\n",
        "for w in list(swearwords):\n",
        "  x,y= countbias(w,FP,TN,FN,TP,test_x_text)\n",
        "  FPD.append((w,x))\n",
        "  FND.append((w,y))\n",
        "\n",
        "FPED=sum([x for _,x in FPD])\n",
        "FNED=sum([x for _,x in FND])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}